{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import csv\n",
    "#from selenium.webdriver.firefox.options import Options\n",
    "#from selenium.webdriver.firefox.service import Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = Options()\n",
    "firefox_driver_path = 'C:/webdrivers/geckodriver.exe'\n",
    "chrome_driver_path = 'C:/webdrivers/chromedriver.exe'\n",
    "service = Service(chrome_driver_path)\n",
    "browser = webdriver.Chrome(options=chrome_options, service=service) # For Chrome\n",
    "#browser = webdriver.Firefox(options=chrome_options, service = service)\n",
    "pwd = os.getcwd()\n",
    "#Empty lists to collect the data from the tables\n",
    "scrape_of_rows_odd = [] \n",
    "scrape_of_rows_even = []\n",
    "\n",
    "urls = [] # List of all the page urls for scraping that we need from 2015 - 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates our list of urls to parse into BS Url. \n",
    "def parse_url():  \n",
    "    year = [*range(2015, 2022)]\n",
    "    year.insert(2, 2016) # Inserting a repeat year as Max drove for both Torro Rosso And Red Bull in 2016\n",
    "    year_check = 0\n",
    "    for y in year:\n",
    "        if y == 2015 or 2016 and year_check < 2 :\n",
    "            constructor = 'toro-rosso'\n",
    "            year_check += 1\n",
    "        else:   \n",
    "            constructor = 'red-bull'\n",
    "        urls.append(f'https://www.statsf1.com/en/{constructor}/grand-prix-{y}.aspx')\n",
    "    return urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the page and gets the data from the table by the get_table_data_by_row() func call\n",
    "def load_page(urls):\n",
    "    parse_url()\n",
    "    for url in urls: \n",
    "        browser.get(url)\n",
    "        soup = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "        get_table_data_by_row(1, 2, 'scrape_of_rows_odd', soup)\n",
    "        get_table_data_by_row(2, 2, 'scrape_of_rows_even', soup)\n",
    "        print(url)\n",
    "    return    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the table data by alternating rows to tackle the row span discrepency\n",
    "def get_table_data_by_row(starting_row, row_increment, file_name, soup_):\n",
    "    row_num = starting_row\n",
    "    while True:\n",
    "        element = soup_.select(f\"tr:nth-of-type({row_num})\")\n",
    "        if len(element) >= 1:\n",
    "            for i in range(0, len(element)):\n",
    "                tr = element[i]\n",
    "                tmp_list = []\n",
    "                td = tr.select('tr > td')\n",
    "                for y in range(0, len(td)):\n",
    "                    tmp_list.append(td[y].text.strip())\n",
    "                with open(pwd + f\"/scraped_data/{file_name}.csv\", 'a', newline='') as file:\n",
    "                    writer = csv.writer(file)\n",
    "                    writer.writerow(tmp_list)    \n",
    "                #list_to_append.append(tmp_list)      \n",
    "            row_num += row_increment\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p align=center>Scraping The Data </p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.statsf1.com/en/toro-rosso/grand-prix-2015.aspx\n",
      "https://www.statsf1.com/en/toro-rosso/grand-prix-2016.aspx\n",
      "https://www.statsf1.com/en/red-bull/grand-prix-2016.aspx\n",
      "https://www.statsf1.com/en/red-bull/grand-prix-2017.aspx\n",
      "https://www.statsf1.com/en/red-bull/grand-prix-2018.aspx\n",
      "https://www.statsf1.com/en/red-bull/grand-prix-2019.aspx\n",
      "https://www.statsf1.com/en/red-bull/grand-prix-2020.aspx\n",
      "https://www.statsf1.com/en/red-bull/grand-prix-2021.aspx\n"
     ]
    }
   ],
   "source": [
    "load_page(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p align=center> Pre Processing Data </p>\n",
    "## <p>Noticing that the data from the website is displayed rather annoyingly, in the form of span rows, I have decided to alternate the rows with the view of pasting together both list into a DataFrame. To acheieve this, I have to pre process the data to have it in the correct sizing, format to import into the DataFrame.  The odd rows in particular, the way the data was displayed resulted in some entries being not of the same length, thus being out of sync with most of the data pulled from this subsection of rows.  The best way I came about to solving this issue, was to isolate the rows that are not needed, in particular the empty lists, and those that took the header of the table, and remove them.  Then insert a generic entry into the lists that remain, to align the data with the rest.  I will go through and clean this data once It is active in the DataFrame</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(172, 11)\n"
     ]
    }
   ],
   "source": [
    "odd_df = pd.DataFrame(scrape_of_rows_odd, columns=[\"id\", \"track\", \"team\", \"driver_num\", \"driver\", \"model\", \"engine\", \"tyre\", \"grid_pos\", \"fin_pos\", \"notes\"]) \n",
    "print(odd_df.shape)\n",
    "\n",
    "odd_df = odd_df.iloc[odd_df.index[46:68]].shift(periods=2, axis=\"columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# Sanity check for the amount of lists with empty strings. \n",
    "len(scrape_of_rows_odd)\n",
    "count = 0\n",
    "for ls in scrape_of_rows_odd: \n",
    "    if ls[0] == '':\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a copy of the list to have a backup incase something goes wrong.\n",
    "tmp_list_of_rows_odd = [row.copy() for row in scrape_of_rows_odd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156\n"
     ]
    }
   ],
   "source": [
    "# Removing lists of empty strings that we have in the list, and the lists of table headers from the webpage, and also insterting generic data into the indices to make it uniform\n",
    "for ls in tmp_list_of_rows_odd:\n",
    "    if ls[0] == '':\n",
    "        tmp_list_of_rows_odd.remove(ls)\n",
    "    if ls[0] == 'n':\n",
    "        tmp_list_of_rows_odd.remove(ls)\n",
    "    if len(ls) <= 10 or len(ls) == 9:\n",
    "        ls.insert(0,'id')\n",
    "        ls.insert(1,'track')\n",
    "        print(ls)\n",
    "    else:\n",
    "        pass\n",
    "print(len(tmp_list_of_rows_odd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for row in tmp_list_of_rows_odd: \n",
    "#     print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniformed_rows_odd = [row.copy() for row in tmp_list_of_rows_odd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the year to the list throughout the seasons results.\n",
    "races_per_season = {\n",
    "    '2015' : \"19\",\n",
    "    '2016' : \"21\",\n",
    "    '20116' : \"21\",\n",
    "    '2017' : \"20\",\n",
    "    '2018' : \"21\",\n",
    "    '2019' : \"21\",\n",
    "    '2020' : \"17\",\n",
    "    '2021' : \"16\"\n",
    "}\n",
    "# Copies some lists.\n",
    "# second_right_list = [[second_right_list[i][0], k, *second_right_list[i][1:]] # Create a new list with the first element in list, then add the key, then unpack the rest from index 1 onwards. \n",
    "#             for k,v in races_per_season.items()\n",
    "#             for i in range(int(v))]\n",
    "\n",
    "i = 0\n",
    "for k,v in races_per_season.items():\n",
    "    total_races = 0\n",
    "    while total_races < int(v):\n",
    "        uniformed_rows_odd[i].insert(1, k)\n",
    "        total_races += 1\n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uniformed_rows_odd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p align=center>Creating the first DataFrame</p>\n",
    "## Now that we have pre processed our odd data, by adding the requried indices to make it conform to the main list we can create our first DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>track</th>\n",
       "      <th>team</th>\n",
       "      <th>driver_num</th>\n",
       "      <th>driver</th>\n",
       "      <th>model</th>\n",
       "      <th>engine</th>\n",
       "      <th>tyre</th>\n",
       "      <th>grid_pos</th>\n",
       "      <th>fin_pos</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>167</td>\n",
       "      <td>2015</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Scuderia Toro Rosso</td>\n",
       "      <td>33</td>\n",
       "      <td>VERSTAPPEN Max</td>\n",
       "      <td>STR10</td>\n",
       "      <td>Renault</td>\n",
       "      <td>Pirelli</td>\n",
       "      <td>11</td>\n",
       "      <td>ab</td>\n",
       "      <td>Engine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>168</td>\n",
       "      <td>2015</td>\n",
       "      <td>Malaysia</td>\n",
       "      <td>Scuderia Toro Rosso</td>\n",
       "      <td>33</td>\n",
       "      <td>VERSTAPPEN Max</td>\n",
       "      <td>STR10</td>\n",
       "      <td>Renault</td>\n",
       "      <td>Pirelli</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>169</td>\n",
       "      <td>2015</td>\n",
       "      <td>China</td>\n",
       "      <td>Scuderia Toro Rosso</td>\n",
       "      <td>33</td>\n",
       "      <td>VERSTAPPEN Max</td>\n",
       "      <td>STR10</td>\n",
       "      <td>Renault</td>\n",
       "      <td>Pirelli</td>\n",
       "      <td>13</td>\n",
       "      <td>17</td>\n",
       "      <td>Engine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>170</td>\n",
       "      <td>2015</td>\n",
       "      <td>Bahrain</td>\n",
       "      <td>Scuderia Toro Rosso</td>\n",
       "      <td>33</td>\n",
       "      <td>VERSTAPPEN Max</td>\n",
       "      <td>STR10</td>\n",
       "      <td>Renault</td>\n",
       "      <td>Pirelli</td>\n",
       "      <td>15</td>\n",
       "      <td>ab</td>\n",
       "      <td>Electrics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>171</td>\n",
       "      <td>2015</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Scuderia Toro Rosso</td>\n",
       "      <td>33</td>\n",
       "      <td>VERSTAPPEN Max</td>\n",
       "      <td>STR10</td>\n",
       "      <td>Renault</td>\n",
       "      <td>Pirelli</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  year      track                 team driver_num          driver  \\\n",
       "0  167  2015  Australia  Scuderia Toro Rosso         33  VERSTAPPEN Max   \n",
       "1  168  2015   Malaysia  Scuderia Toro Rosso         33  VERSTAPPEN Max   \n",
       "2  169  2015      China  Scuderia Toro Rosso         33  VERSTAPPEN Max   \n",
       "3  170  2015    Bahrain  Scuderia Toro Rosso         33  VERSTAPPEN Max   \n",
       "4  171  2015      Spain  Scuderia Toro Rosso         33  VERSTAPPEN Max   \n",
       "\n",
       "   model   engine     tyre grid_pos fin_pos      notes  \n",
       "0  STR10  Renault  Pirelli       11      ab     Engine  \n",
       "1  STR10  Renault  Pirelli        6       7             \n",
       "2  STR10  Renault  Pirelli       13      17     Engine  \n",
       "3  STR10  Renault  Pirelli       15      ab  Electrics  \n",
       "4  STR10  Renault  Pirelli        6      11             "
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd_df = pd.DataFrame(uniformed_rows_odd, columns=[\"id\",\"year\", \"track\", \"team\", \"driver_num\", \"driver\", \"model\", \"engine\", \"tyre\", \"grid_pos\", \"fin_pos\", \"notes\"]) \n",
    "odd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_scrape_of_rows_even = [row.copy() for row in scrape_of_rows_even] # Make a copy of our scraped data to fall back on in case of errors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in tmp_scrape_of_rows_even:\n",
    "    #print(row)\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6180266b96e5d68251ba877e2f7e9407e638de689775c4774b9b7388b1dfb47e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
